# Алгоритмы
1. временная сложность (time complexity) — это максимальное возможное количество выполненных алгоритмом элементарных операций
1. ёмкостная сложность (space complexity) — это максимальный возможный размер занятой алгоритмом дополнительной памяти, как функция от размера входных данных.
1. Сложность дает понятие о масштабе сложности, а не о точном его значении
1. Тут три ключевых части: Это функция. Зависит от размера входных данных. Возвращает максимум операций/памяти на всех входах такого размера. То есть **для худшего, самого затратного случая**.
   
   Пример
   ```
    for (let i=0; i < n; i++)
    count++;
    ```
    Посчитаем количество элементарных операций:
    ```
    1 для int i = 0
    n+1 для i < n
    2n для i++ (что эквивалентно i = i + 1, а это две операции: присваивание и сложение)
    2n для сount++
   ```
    Получаем, что временную сложность алгоритма C(n) = 2 + 5nC(n)=2+5n.

## Асимптотическая оценка
1. Если кратко и грубо, то получить асимптотическую оценку можно так: отбросьте в функции сложности все слагаемые, кроме одного с самой быстрой скоростью роста. А потом отбросьте все константы. То что получится и будет асимптотической оценкой сложности.
1. Обычно время работы алгоритма нас интересует на больших данных, когда алгоритм может существенно замедлиться. Асимптотическая оценка как раз отвечает на вопрос, как сильно деградирует производительность с ростом размера входа.

## O-нотация
1. O(n²) (произносится: о от эн квадрат), Θ(n log(n)) (произносится: тэта от эн лог эн).
1. Если очень кратко — O(n²) означает, что **самое быстро растущее слагаемое** в функции сложности — это n в степени не более 2, 
1. Θ(n²) — что степень в точности 2.

### Неформальные трактовки
1. «f = O(g)» можно трактовать так: «функция f(n) растет не быстрее, чем функция g(n) с ростом n» (Знак равенства в записи «f = O(g)» не настоящее равенство.)
1. «f = Θ(g)» — это соответственно «f растет так же быстро, как и g (если игнорировать различия в константном множителе)».
1. При беглом анализе алгоритма, нам не нужна функция сложности, а достаточно лишь её оценки.

    Пример
    ```
    let i = 1;
    while (i < n) {
      for(let j = 0; j < n-2; j++)
        count++;
      i++;
    }
    ```
1. Необходимо найти самую «горячую» операцию — ту, которая выполняется больше всего раз. Чаще всего такая операция находится внутри самого вложенного цикла. 
    Такая операция count++ или j++ внутри for
1. Заметим, что count++ выполняется чуть меньше n² раз. Не точно n², потому что i начинается с 1, а не с нуля, да и внутренний цикл до n-2.  
    В этом случае, например, точное количество выполнений count++ — это (n-1)*(n-2) = Θ(n²).
    Все остальные операции не изменят асимптотику, поэтому их можно даже не рассматривать.
    Значит оценка сложности этого кода и есть Θ(n²).
1. Благодаря переходу от функции сложности к асимптотической оценке, процесс анализа алгоритма становится действительно быстрым и простым.
    Мы оцениваем сложность алгоритма, чтобы убедиться, что он не будет «тормозить». 
    Асимптотическая оценка даёт нам примерное представление, при каком размере задачи (того самого n, от которого зависит функция сложности), можно ожидать приемлемой скорости выполнения.


var count = 0;
for (int i = 1; i < n; i *= 2)
    count++; 
    
    =Θ(log n)
     Правильно! удвоить i до тех пор, пока значение не станет больше n, получится не более log(n)+1 раз
    
Удвоение переменной цикла делает цикл логарифмическим по сложности. В таких случаях для оценки сложности помогают основные свойства логарифмов.

При смене основания добавляется константный множитель: \log_a n = C*\log_b nlog 
a
​	
 n=C∗log 
b
​	
 n. Поэтому под символом Θ или O основание у логарифма опускают. Точно также как опускают константные множители.
Степень аргумента логарифма можно перекинуть в множитель перед логарифмом: \log n^2 = 2 \log nlogn 
2
 =2logn. Как следствие, под символами Θ и O степень у аргумента логарифма можно отбросить — она эквивалентна умножению на константу.
Очень просто запомнить — степень аргумента и основание логарифма смело выкидывайте из рассмотрения! Давайте попробуем это в деле:

var count = 0;
for (int i = 1; i < n*n; i *= 3)
    count++;
    
    = Θ(log n)
      Правильно! *=3 сменит основание логарифма, а n*n даст вторую степень у аргумента. Всё это можно смело отбросить!
      
https://stackoverflow.com/questions/2307283/what-does-olog-n-mean-exactly


## амортизационная оценка сложности
1. это асимптотическая оценка усреднённого количество элементарных операций, которое выполняется при каждом вызове метода.
```
class DynamicArray
{
	public int Count = 0;
	public int[] Items = new int[1];

	public void Add(int x)
	{
		if (Count >= Items.Length)
		{
			int[] newItems = new int[2 * Items.Length];
			for (int j = 0; j < Count; j++)
				newItems[j] = Items[j];
			Items = newItems;
		}
		Items[Count] = x;
		Count++;
	}

	public void AddRange(int n)
	{
		for (int i = 0; i < n; i++)
			Add(i);
	}
}
```
1. Сложность у метода AddRange Θ(n), однако в среднем Add работает как бы за Θ(1), потому что копирование случается очень редко.
1. Более корректно говорить так: при последовательных вызовах Add, этот метод имеет амортизационную оценку сложности Θ(1).
1. Паттерн «расти в n раз» важен.
```
List<int> GetCompoundNumbers(int n)
{
	var list = new List<int>();
	for (int i = 0; i < n; i++)
		for (int j = 0; j < i; j++)
			list.Add(i * j);
	return list;
}
```
list.Add выполнится n² раз, но амортизационная оценка Add в данном случае — Θ(1)
